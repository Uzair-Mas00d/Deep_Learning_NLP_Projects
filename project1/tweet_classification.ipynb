{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making pipeline for cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns_to_drop)\n",
    "    \n",
    "def process_text_column(text_column):\n",
    "    return text_column.replace('[^a-zA-Z0-9\" \"]', '', regex=True)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('drop_columns', DropColumns(columns_to_drop=['keyword', 'location', 'id'])),\n",
    "    ('process_text', FunctionTransformer(func=process_text_column, validate=False))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using pipeline to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this earthquake Ma...       1\n",
       "1              Forest fire near La Ronge Sask Canada       1\n",
       "2  All residents asked to shelter in place are be...       1\n",
       "3  13000 people receive wildfires evacuation orde...       1\n",
       "4  Just got sent this photo from Ruby Alaska as s...       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pipeline.transform(train_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting text into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].apply(lambda x: \" \".join(word.lower() for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Counting number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22546"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(train_df.text)\n",
    "num_of_unique_words = len(counter)\n",
    "num_of_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'in': 3,\n",
       " 'to': 4,\n",
       " 'of': 5,\n",
       " 'and': 6,\n",
       " 'i': 7,\n",
       " 'is': 8,\n",
       " 'for': 9,\n",
       " 'on': 10,\n",
       " 'you': 11,\n",
       " 'my': 12,\n",
       " 'with': 13,\n",
       " 'it': 14,\n",
       " 'that': 15,\n",
       " 'at': 16,\n",
       " 'by': 17,\n",
       " 'this': 18,\n",
       " 'from': 19,\n",
       " 'be': 20,\n",
       " 'are': 21,\n",
       " 'have': 22,\n",
       " 'was': 23,\n",
       " 'like': 24,\n",
       " 'as': 25,\n",
       " 'up': 26,\n",
       " 'just': 27,\n",
       " 'so': 28,\n",
       " 'me': 29,\n",
       " 'but': 30,\n",
       " 'im': 31,\n",
       " 'amp': 32,\n",
       " 'not': 33,\n",
       " 'your': 34,\n",
       " 'its': 35,\n",
       " 'out': 36,\n",
       " 'after': 37,\n",
       " 'will': 38,\n",
       " 'all': 39,\n",
       " 'when': 40,\n",
       " 'no': 41,\n",
       " 'an': 42,\n",
       " 'fire': 43,\n",
       " 'has': 44,\n",
       " 'if': 45,\n",
       " 'we': 46,\n",
       " 'get': 47,\n",
       " 'new': 48,\n",
       " 'via': 49,\n",
       " 'now': 50,\n",
       " 'more': 51,\n",
       " 'about': 52,\n",
       " 'dont': 53,\n",
       " 'or': 54,\n",
       " 'what': 55,\n",
       " 'people': 56,\n",
       " 'he': 57,\n",
       " 'they': 58,\n",
       " 'been': 59,\n",
       " 'one': 60,\n",
       " 'how': 61,\n",
       " 'over': 62,\n",
       " 'news': 63,\n",
       " 'who': 64,\n",
       " 'into': 65,\n",
       " 'do': 66,\n",
       " 'were': 67,\n",
       " 'video': 68,\n",
       " 'us': 69,\n",
       " '2': 70,\n",
       " 'can': 71,\n",
       " 'emergency': 72,\n",
       " 'disaster': 73,\n",
       " 'there': 74,\n",
       " 'police': 75,\n",
       " 'than': 76,\n",
       " 'her': 77,\n",
       " 'would': 78,\n",
       " 'some': 79,\n",
       " 'still': 80,\n",
       " 'his': 81,\n",
       " 'body': 82,\n",
       " 'burning': 83,\n",
       " 'back': 84,\n",
       " 'california': 85,\n",
       " 'crash': 86,\n",
       " 'suicide': 87,\n",
       " 'storm': 88,\n",
       " 'off': 89,\n",
       " 'got': 90,\n",
       " 'time': 91,\n",
       " 'them': 92,\n",
       " 'why': 93,\n",
       " 'had': 94,\n",
       " 'know': 95,\n",
       " 'man': 96,\n",
       " 'cant': 97,\n",
       " 'first': 98,\n",
       " 'buildings': 99,\n",
       " 'day': 100,\n",
       " 'rt': 101,\n",
       " 'going': 102,\n",
       " 'bomb': 103,\n",
       " 'world': 104,\n",
       " 'nuclear': 105,\n",
       " 'see': 106,\n",
       " 'our': 107,\n",
       " 'love': 108,\n",
       " 'fires': 109,\n",
       " 'attack': 110,\n",
       " 'two': 111,\n",
       " '3': 112,\n",
       " 'youtube': 113,\n",
       " 'go': 114,\n",
       " 'killed': 115,\n",
       " 'dead': 116,\n",
       " 'their': 117,\n",
       " 'train': 118,\n",
       " 'full': 119,\n",
       " 'war': 120,\n",
       " 'being': 121,\n",
       " 'car': 122,\n",
       " 'only': 123,\n",
       " 'good': 124,\n",
       " 'families': 125,\n",
       " 'may': 126,\n",
       " 'accident': 127,\n",
       " 'today': 128,\n",
       " 'think': 129,\n",
       " 'say': 130,\n",
       " 'down': 131,\n",
       " 'hiroshima': 132,\n",
       " 'life': 133,\n",
       " 'many': 134,\n",
       " 'watch': 135,\n",
       " 'last': 136,\n",
       " 'here': 137,\n",
       " 'u': 138,\n",
       " 'could': 139,\n",
       " 'years': 140,\n",
       " 'want': 141,\n",
       " 'too': 142,\n",
       " 'make': 143,\n",
       " 'then': 144,\n",
       " 'way': 145,\n",
       " 'did': 146,\n",
       " 'collapse': 147,\n",
       " 'home': 148,\n",
       " 'work': 149,\n",
       " 'because': 150,\n",
       " 'look': 151,\n",
       " 'help': 152,\n",
       " 'need': 153,\n",
       " 'wildfire': 154,\n",
       " 'am': 155,\n",
       " 'even': 156,\n",
       " 'best': 157,\n",
       " 'really': 158,\n",
       " 'death': 159,\n",
       " 'army': 160,\n",
       " 'mh370': 161,\n",
       " 'mass': 162,\n",
       " 'year': 163,\n",
       " 'him': 164,\n",
       " 'take': 165,\n",
       " 'youre': 166,\n",
       " 'please': 167,\n",
       " 'pm': 168,\n",
       " 'another': 169,\n",
       " 'right': 170,\n",
       " 'should': 171,\n",
       " 'those': 172,\n",
       " 'lol': 173,\n",
       " 'bombing': 174,\n",
       " '4': 175,\n",
       " 'hot': 176,\n",
       " 'forest': 177,\n",
       " 'school': 178,\n",
       " 'black': 179,\n",
       " 'much': 180,\n",
       " '5': 181,\n",
       " 'northern': 182,\n",
       " 'fatal': 183,\n",
       " 'read': 184,\n",
       " 'water': 185,\n",
       " '1': 186,\n",
       " 'obama': 187,\n",
       " 'she': 188,\n",
       " 'legionnaires': 189,\n",
       " 'great': 190,\n",
       " 'never': 191,\n",
       " 'wreck': 192,\n",
       " 'let': 193,\n",
       " 'homes': 194,\n",
       " 'bomber': 195,\n",
       " 'live': 196,\n",
       " 'city': 197,\n",
       " 'latest': 198,\n",
       " 'every': 199,\n",
       " 'any': 200,\n",
       " '2015': 201,\n",
       " 'thats': 202,\n",
       " 'old': 203,\n",
       " 's': 204,\n",
       " 'atomic': 205,\n",
       " 'said': 206,\n",
       " 'flames': 207,\n",
       " 'floods': 208,\n",
       " 'getting': 209,\n",
       " 'shit': 210,\n",
       " 'come': 211,\n",
       " 'where': 212,\n",
       " 'feel': 213,\n",
       " 'under': 214,\n",
       " 'fear': 215,\n",
       " 'near': 216,\n",
       " 'flood': 217,\n",
       " 'top': 218,\n",
       " 'while': 219,\n",
       " 'ever': 220,\n",
       " 'content': 221,\n",
       " 'most': 222,\n",
       " 'japan': 223,\n",
       " 'oil': 224,\n",
       " 'damage': 225,\n",
       " 'everyone': 226,\n",
       " 'found': 227,\n",
       " 'hit': 228,\n",
       " 'since': 229,\n",
       " 'military': 230,\n",
       " 'coming': 231,\n",
       " 'before': 232,\n",
       " 'state': 233,\n",
       " 'during': 234,\n",
       " 'weather': 235,\n",
       " 'without': 236,\n",
       " 'hope': 237,\n",
       " 'injured': 238,\n",
       " 'ass': 239,\n",
       " 'earthquake': 240,\n",
       " 'flooding': 241,\n",
       " 'next': 242,\n",
       " 'god': 243,\n",
       " 'stop': 244,\n",
       " 'debris': 245,\n",
       " 'well': 246,\n",
       " 'malaysia': 247,\n",
       " 'evacuation': 248,\n",
       " 'night': 249,\n",
       " 'these': 250,\n",
       " 'truck': 251,\n",
       " 'which': 252,\n",
       " 'smoke': 253,\n",
       " 'set': 254,\n",
       " 'times': 255,\n",
       " 'little': 256,\n",
       " 'cause': 257,\n",
       " 'wounded': 258,\n",
       " 'plan': 259,\n",
       " 'ive': 260,\n",
       " 'food': 261,\n",
       " 'movie': 262,\n",
       " 'through': 263,\n",
       " 'cross': 264,\n",
       " 'confirmed': 265,\n",
       " 'thunderstorm': 266,\n",
       " 'severe': 267,\n",
       " 'always': 268,\n",
       " 'w': 269,\n",
       " 'fucking': 270,\n",
       " 'looks': 271,\n",
       " 'wild': 272,\n",
       " 'reddit': 273,\n",
       " 'theres': 274,\n",
       " 'heat': 275,\n",
       " 'face': 276,\n",
       " 'hes': 277,\n",
       " 'bad': 278,\n",
       " 'natural': 279,\n",
       " 'rain': 280,\n",
       " 'until': 281,\n",
       " 'bloody': 282,\n",
       " 'family': 283,\n",
       " 'services': 284,\n",
       " 'liked': 285,\n",
       " 'says': 286,\n",
       " 'fall': 287,\n",
       " 'weapons': 288,\n",
       " 'injuries': 289,\n",
       " 'thunder': 290,\n",
       " 'gonna': 291,\n",
       " 'also': 292,\n",
       " 'screaming': 293,\n",
       " 'blood': 294,\n",
       " '70': 295,\n",
       " 'boy': 296,\n",
       " 'high': 297,\n",
       " 'warning': 298,\n",
       " 'missing': 299,\n",
       " 'weapon': 300,\n",
       " 'lightning': 301,\n",
       " 'bags': 302,\n",
       " 'sinking': 303,\n",
       " 'evacuate': 304,\n",
       " 'spill': 305,\n",
       " 'refugees': 306,\n",
       " 'loud': 307,\n",
       " 'summer': 308,\n",
       " 'end': 309,\n",
       " 'check': 310,\n",
       " 'house': 311,\n",
       " 'run': 312,\n",
       " 'change': 313,\n",
       " 'again': 314,\n",
       " 'made': 315,\n",
       " 'trapped': 316,\n",
       " 'collided': 317,\n",
       " 'photo': 318,\n",
       " 'head': 319,\n",
       " 'free': 320,\n",
       " 'murder': 321,\n",
       " 'destroy': 322,\n",
       " 'whole': 323,\n",
       " 'survive': 324,\n",
       " 'hail': 325,\n",
       " 'released': 326,\n",
       " 'attacked': 327,\n",
       " 'bag': 328,\n",
       " 'failure': 329,\n",
       " 'deaths': 330,\n",
       " 'migrants': 331,\n",
       " 'wreckage': 332,\n",
       " 'sinkhole': 333,\n",
       " 'outbreak': 334,\n",
       " 'area': 335,\n",
       " 'does': 336,\n",
       " 'explosion': 337,\n",
       " 'someone': 338,\n",
       " 'air': 339,\n",
       " 'save': 340,\n",
       " 'destroyed': 341,\n",
       " 'saudi': 342,\n",
       " 'hurricane': 343,\n",
       " 'bridge': 344,\n",
       " 'derailment': 345,\n",
       " 'rescue': 346,\n",
       " 'panic': 347,\n",
       " 'fatalities': 348,\n",
       " 'breaking': 349,\n",
       " 'burned': 350,\n",
       " 'trauma': 351,\n",
       " 'ambulance': 352,\n",
       " 'charged': 353,\n",
       " 'fuck': 354,\n",
       " 'big': 355,\n",
       " 'post': 356,\n",
       " 'terrorist': 357,\n",
       " 'keep': 358,\n",
       " 'wind': 359,\n",
       " 'report': 360,\n",
       " 'rescued': 361,\n",
       " 'wrecked': 362,\n",
       " 'county': 363,\n",
       " 'around': 364,\n",
       " 'tonight': 365,\n",
       " 'road': 366,\n",
       " 'island': 367,\n",
       " 'survivors': 368,\n",
       " 'destruction': 369,\n",
       " 'lets': 370,\n",
       " 'against': 371,\n",
       " 'girl': 372,\n",
       " 'lives': 373,\n",
       " 'explode': 374,\n",
       " 'ruin': 375,\n",
       " 'service': 376,\n",
       " 'catastrophe': 377,\n",
       " 'survived': 378,\n",
       " 'dust': 379,\n",
       " 'twister': 380,\n",
       " 'other': 381,\n",
       " 'bus': 382,\n",
       " 'week': 383,\n",
       " '40': 384,\n",
       " 'call': 385,\n",
       " 'show': 386,\n",
       " 'violent': 387,\n",
       " 'white': 388,\n",
       " 'real': 389,\n",
       " 'story': 390,\n",
       " 'put': 391,\n",
       " 'boat': 392,\n",
       " 'drought': 393,\n",
       " 'collapsed': 394,\n",
       " 'harm': 395,\n",
       " 'landslide': 396,\n",
       " 'danger': 397,\n",
       " 'deluge': 398,\n",
       " 'investigators': 399,\n",
       " 'quarantine': 400,\n",
       " 'structural': 401,\n",
       " 'sandstorm': 402,\n",
       " 'whirlwind': 403,\n",
       " 'update': 404,\n",
       " 'least': 405,\n",
       " 'injury': 406,\n",
       " 'ill': 407,\n",
       " 'came': 408,\n",
       " 'suspect': 409,\n",
       " 'crashed': 410,\n",
       " 'phone': 411,\n",
       " 'red': 412,\n",
       " 'armageddon': 413,\n",
       " 'saw': 414,\n",
       " 'things': 415,\n",
       " 're': 416,\n",
       " 'rescuers': 417,\n",
       " 'rioting': 418,\n",
       " 'riot': 419,\n",
       " 'hazard': 420,\n",
       " 'curfew': 421,\n",
       " 'devastation': 422,\n",
       " 'drowning': 423,\n",
       " 'engulfed': 424,\n",
       " 'screamed': 425,\n",
       " 'hostages': 426,\n",
       " 'mudslide': 427,\n",
       " 'quarantined': 428,\n",
       " 'sunk': 429,\n",
       " 'wave': 430,\n",
       " 'better': 431,\n",
       " 'horrible': 432,\n",
       " 'past': 433,\n",
       " 'heard': 434,\n",
       " 'game': 435,\n",
       " 'thing': 436,\n",
       " 'airplane': 437,\n",
       " 'ok': 438,\n",
       " 'kills': 439,\n",
       " 'iran': 440,\n",
       " 'national': 441,\n",
       " 'id': 442,\n",
       " 'mosque': 443,\n",
       " 'woman': 444,\n",
       " 'fedex': 445,\n",
       " 'wanna': 446,\n",
       " 'bleeding': 447,\n",
       " 'tragedy': 448,\n",
       " 'anniversary': 449,\n",
       " 'bombed': 450,\n",
       " 'displaced': 451,\n",
       " 'traumatised': 452,\n",
       " 'casualties': 453,\n",
       " 'hazardous': 454,\n",
       " 'hundreds': 455,\n",
       " 'stock': 456,\n",
       " 'derail': 457,\n",
       " 'exploded': 458,\n",
       " 'inundated': 459,\n",
       " 'windstorm': 460,\n",
       " 'heart': 461,\n",
       " 'long': 462,\n",
       " 'part': 463,\n",
       " 'twitter': 464,\n",
       " 'group': 465,\n",
       " 'battle': 466,\n",
       " 'women': 467,\n",
       " 'oh': 468,\n",
       " 'blew': 469,\n",
       " 'drown': 470,\n",
       " 'blown': 471,\n",
       " 'cliff': 472,\n",
       " 'crush': 473,\n",
       " 'wounds': 474,\n",
       " 'detonate': 475,\n",
       " 'electrocuted': 476,\n",
       " 'famine': 477,\n",
       " 'bang': 478,\n",
       " 'lava': 479,\n",
       " '15': 480,\n",
       " 'reunion': 481,\n",
       " 'ebay': 482,\n",
       " 'lot': 483,\n",
       " 'must': 484,\n",
       " 'went': 485,\n",
       " 'august': 486,\n",
       " 'away': 487,\n",
       " 'power': 488,\n",
       " 'bioterror': 489,\n",
       " 'hostage': 490,\n",
       " 'half': 491,\n",
       " 'catastrophic': 492,\n",
       " 'affected': 493,\n",
       " 'chemical': 494,\n",
       " 'collide': 495,\n",
       " 'typhoon': 496,\n",
       " 'derailed': 497,\n",
       " 'desolation': 498,\n",
       " 'trouble': 499,\n",
       " 'evacuated': 500,\n",
       " 'flattened': 501,\n",
       " 'hijacker': 502,\n",
       " 'panicking': 503,\n",
       " 'razed': 504,\n",
       " 'screams': 505,\n",
       " 'thank': 506,\n",
       " 'left': 507,\n",
       " '9': 508,\n",
       " 'very': 509,\n",
       " 'possible': 510,\n",
       " 'plane': 511,\n",
       " 'soon': 512,\n",
       " 'apocalypse': 513,\n",
       " 'calgary': 514,\n",
       " 'market': 515,\n",
       " 'land': 516,\n",
       " 'send': 517,\n",
       " 'baby': 518,\n",
       " 'demolish': 519,\n",
       " 'demolition': 520,\n",
       " 'hijacking': 521,\n",
       " 'massacre': 522,\n",
       " 'tornado': 523,\n",
       " 'cool': 524,\n",
       " 'care': 525,\n",
       " 'something': 526,\n",
       " 'traffic': 527,\n",
       " 'use': 528,\n",
       " 'didnt': 529,\n",
       " 'meltdown': 530,\n",
       " 'doing': 531,\n",
       " 'goes': 532,\n",
       " 'annihilated': 533,\n",
       " 'thought': 534,\n",
       " 'zone': 535,\n",
       " 'kill': 536,\n",
       " 'terrorism': 537,\n",
       " 'sure': 538,\n",
       " 'longer': 539,\n",
       " 'security': 540,\n",
       " 'song': 541,\n",
       " 'light': 542,\n",
       " 'bagging': 543,\n",
       " 'pkk': 544,\n",
       " 'caused': 545,\n",
       " 'collision': 546,\n",
       " 'detonation': 547,\n",
       " 'pandemonium': 548,\n",
       " 'detonated': 549,\n",
       " 'due': 550,\n",
       " 'st': 551,\n",
       " 'used': 552,\n",
       " 'issues': 553,\n",
       " 'minute': 554,\n",
       " 'government': 555,\n",
       " 'airport': 556,\n",
       " 'river': 557,\n",
       " 'tomorrow': 558,\n",
       " 'fan': 559,\n",
       " 'arson': 560,\n",
       " 'sound': 561,\n",
       " 'yet': 562,\n",
       " 'beautiful': 563,\n",
       " 'shoulder': 564,\n",
       " '6': 565,\n",
       " 'responders': 566,\n",
       " 'blast': 567,\n",
       " 'demolished': 568,\n",
       " 'drowned': 569,\n",
       " 'volcano': 570,\n",
       " 'tsunami': 571,\n",
       " 'eyewitness': 572,\n",
       " 'fatality': 573,\n",
       " 'prebreak': 574,\n",
       " 'obliterate': 575,\n",
       " 'obliterated': 576,\n",
       " 'building': 577,\n",
       " 'three': 578,\n",
       " 'thanks': 579,\n",
       " 'kids': 580,\n",
       " 'shooting': 581,\n",
       " 'few': 582,\n",
       " 'already': 583,\n",
       " 'making': 584,\n",
       " 'sirens': 585,\n",
       " 'fun': 586,\n",
       " '8': 587,\n",
       " 'remember': 588,\n",
       " 'blazing': 589,\n",
       " 'nothing': 590,\n",
       " 'ur': 591,\n",
       " 'officer': 592,\n",
       " 'crushed': 593,\n",
       " 'hellfire': 594,\n",
       " 'murderer': 595,\n",
       " 'obliteration': 596,\n",
       " 'died': 597,\n",
       " 'far': 598,\n",
       " 'south': 599,\n",
       " 'days': 600,\n",
       " 'ablaze': 601,\n",
       " 'inside': 602,\n",
       " 'leave': 603,\n",
       " 'actually': 604,\n",
       " 'same': 605,\n",
       " 'done': 606,\n",
       " 'believe': 607,\n",
       " 'wake': 608,\n",
       " 'hell': 609,\n",
       " 'fight': 610,\n",
       " 'stay': 611,\n",
       " 'officials': 612,\n",
       " 'israeli': 613,\n",
       " 'media': 614,\n",
       " 'person': 615,\n",
       " 'guys': 616,\n",
       " 'music': 617,\n",
       " 'having': 618,\n",
       " 'words': 619,\n",
       " 'policy': 620,\n",
       " 'turkey': 621,\n",
       " 'bush': 622,\n",
       " 'cyclone': 623,\n",
       " 'hijack': 624,\n",
       " 'sue': 625,\n",
       " 'rainstorm': 626,\n",
       " '16yr': 627,\n",
       " 'upheaval': 628,\n",
       " 'both': 629,\n",
       " 'second': 630,\n",
       " 'whats': 631,\n",
       " 'site': 632,\n",
       " 'shot': 633,\n",
       " 'north': 634,\n",
       " 'plans': 635,\n",
       " 'bc': 636,\n",
       " 'hours': 637,\n",
       " 'start': 638,\n",
       " 'avalanche': 639,\n",
       " 'lab': 640,\n",
       " 'doesnt': 641,\n",
       " 'such': 642,\n",
       " 'nearby': 643,\n",
       " 'declares': 644,\n",
       " 'seismic': 645,\n",
       " 'electrocute': 646,\n",
       " 'la': 647,\n",
       " 'place': 648,\n",
       " '20': 649,\n",
       " 'wait': 650,\n",
       " 'nowplaying': 651,\n",
       " 'gets': 652,\n",
       " 'yourself': 653,\n",
       " 'brown': 654,\n",
       " 'play': 655,\n",
       " 'men': 656,\n",
       " 'ago': 657,\n",
       " 'wont': 658,\n",
       " 'horror': 659,\n",
       " 'peace': 660,\n",
       " 'india': 661,\n",
       " 'trying': 662,\n",
       " '7': 663,\n",
       " 'health': 664,\n",
       " 'yes': 665,\n",
       " 'pic': 666,\n",
       " 'mp': 667,\n",
       " 'casualty': 668,\n",
       " 'isis': 669,\n",
       " 'deluged': 670,\n",
       " 'offensive': 671,\n",
       " 'rubble': 672,\n",
       " 'swallowed': 673,\n",
       " 'stretcher': 674,\n",
       " 'outside': 675,\n",
       " 'n': 676,\n",
       " 'tell': 677,\n",
       " 'job': 678,\n",
       " 'almost': 679,\n",
       " 'aircraft': 680,\n",
       " 'helicopter': 681,\n",
       " 'history': 682,\n",
       " 'data': 683,\n",
       " 'own': 684,\n",
       " 'children': 685,\n",
       " 'anything': 686,\n",
       " 'american': 687,\n",
       " 'business': 688,\n",
       " 'deal': 689,\n",
       " 'line': 690,\n",
       " 'watching': 691,\n",
       " 'bigger': 692,\n",
       " 'abc': 693,\n",
       " 'typhoondevastated': 694,\n",
       " 'saipan': 695,\n",
       " 'reactor': 696,\n",
       " 'snowstorm': 697,\n",
       " 'conclusively': 698,\n",
       " 'street': 699,\n",
       " 'west': 700,\n",
       " '30': 701,\n",
       " 'support': 702,\n",
       " '12': 703,\n",
       " 'reuters': 704,\n",
       " 'bar': 705,\n",
       " 'maybe': 706,\n",
       " 'order': 707,\n",
       " 'pick': 708,\n",
       " 'makes': 709,\n",
       " 'yeah': 710,\n",
       " '50': 711,\n",
       " 'transport': 712,\n",
       " 'blight': 713,\n",
       " 'searching': 714,\n",
       " 'low': 715,\n",
       " 'money': 716,\n",
       " 'photos': 717,\n",
       " 'find': 718,\n",
       " 'm': 719,\n",
       " 'memories': 720,\n",
       " 'hear': 721,\n",
       " 'rise': 722,\n",
       " 'waves': 723,\n",
       " 'bodies': 724,\n",
       " 'projected': 725,\n",
       " 'siren': 726,\n",
       " 'bestnaijamade': 727,\n",
       " 'lost': 728,\n",
       " 'd': 729,\n",
       " 'finally': 730,\n",
       " 'book': 731,\n",
       " 'die': 732,\n",
       " 'anyone': 733,\n",
       " 'theyre': 734,\n",
       " 'might': 735,\n",
       " 'everything': 736,\n",
       " 'hey': 737,\n",
       " '10': 738,\n",
       " 'tv': 739,\n",
       " 'control': 740,\n",
       " 'amid': 741,\n",
       " 'team': 742,\n",
       " 'feeling': 743,\n",
       " 'literally': 744,\n",
       " 'bioterrorism': 745,\n",
       " 'hollywood': 746,\n",
       " 'pretty': 747,\n",
       " 'move': 748,\n",
       " 'online': 749,\n",
       " 'probably': 750,\n",
       " 'myself': 751,\n",
       " 'saved': 752,\n",
       " 'crews': 753,\n",
       " 'signs': 754,\n",
       " 'effect': 755,\n",
       " 'manslaughter': 756,\n",
       " 'trench': 757,\n",
       " 'chinas': 758,\n",
       " 'fast': 759,\n",
       " 'side': 760,\n",
       " 'happy': 761,\n",
       " 'center': 762,\n",
       " 'shes': 763,\n",
       " 'feared': 764,\n",
       " 'country': 765,\n",
       " 'seen': 766,\n",
       " 'annihilation': 767,\n",
       " 'name': 768,\n",
       " 'b': 769,\n",
       " 'crisis': 770,\n",
       " 'isnt': 771,\n",
       " 'leather': 772,\n",
       " 'caught': 773,\n",
       " 'town': 774,\n",
       " 'blaze': 775,\n",
       " 'okay': 776,\n",
       " 'damn': 777,\n",
       " 'space': 778,\n",
       " 'though': 779,\n",
       " 'spot': 780,\n",
       " 'trains': 781,\n",
       " 'soudelor': 782,\n",
       " 'desolate': 783,\n",
       " 'hat': 784,\n",
       " 'refugio': 785,\n",
       " 'costlier': 786,\n",
       " 'miners': 787,\n",
       " 'flag': 788,\n",
       " 'cars': 789,\n",
       " 'others': 790,\n",
       " 'huge': 791,\n",
       " 'once': 792,\n",
       " 'daily': 793,\n",
       " 'jobs': 794,\n",
       " 't': 795,\n",
       " 'eyes': 796,\n",
       " 'omg': 797,\n",
       " 'ship': 798,\n",
       " 'crazy': 799,\n",
       " '11': 800,\n",
       " 'hate': 801,\n",
       " 'ball': 802,\n",
       " 'case': 803,\n",
       " 'self': 804,\n",
       " 'major': 805,\n",
       " 'child': 806,\n",
       " 'called': 807,\n",
       " 'class': 808,\n",
       " 'womens': 809,\n",
       " 'texas': 810,\n",
       " 'youth': 811,\n",
       " 'needs': 812,\n",
       " 'nearly': 813,\n",
       " 'giant': 814,\n",
       " 'course': 815,\n",
       " 'banned': 816,\n",
       " '11yearold': 817,\n",
       " 'picking': 818,\n",
       " 'hailstorm': 819,\n",
       " 'closed': 820,\n",
       " 'heavy': 821,\n",
       " 'flash': 822,\n",
       " 'across': 823,\n",
       " 'haha': 824,\n",
       " 'lord': 825,\n",
       " 'hard': 826,\n",
       " 'rd': 827,\n",
       " 'mom': 828,\n",
       " 'win': 829,\n",
       " 'wrong': 830,\n",
       " 'official': 831,\n",
       " 'pakistan': 832,\n",
       " 'sorry': 833,\n",
       " 'usa': 834,\n",
       " 'stand': 835,\n",
       " 'toddler': 836,\n",
       " 'united': 837,\n",
       " 'east': 838,\n",
       " 'gun': 839,\n",
       " 'worst': 840,\n",
       " 'listen': 841,\n",
       " 'anthrax': 842,\n",
       " 'computers': 843,\n",
       " 'entire': 844,\n",
       " 'level': 845,\n",
       " 'morning': 846,\n",
       " 'meek': 847,\n",
       " 'russian': 848,\n",
       " '60': 849,\n",
       " 'aug': 850,\n",
       " 'gbbo': 851,\n",
       " 'houses': 852,\n",
       " 'become': 853,\n",
       " 'chance': 854,\n",
       " 'angry': 855,\n",
       " 'ignition': 856,\n",
       " 'knock': 857,\n",
       " 'mayhem': 858,\n",
       " 'reason': 859,\n",
       " 'try': 860,\n",
       " 'yours': 861,\n",
       " 'talk': 862,\n",
       " 'vehicle': 863,\n",
       " 'aftershock': 864,\n",
       " 'guy': 865,\n",
       " 'couple': 866,\n",
       " 'poor': 867,\n",
       " 'radio': 868,\n",
       " 'totally': 869,\n",
       " 'gt': 870,\n",
       " 'blue': 871,\n",
       " 'learn': 872,\n",
       " 'beach': 873,\n",
       " 'christian': 874,\n",
       " 'temple': 875,\n",
       " 'view': 876,\n",
       " 'star': 877,\n",
       " 'taken': 878,\n",
       " 'playing': 879,\n",
       " 'mishaps': 880,\n",
       " 'action': 881,\n",
       " 'public': 882,\n",
       " 'running': 883,\n",
       " 'looking': 884,\n",
       " 'cake': 885,\n",
       " 'pain': 886,\n",
       " '25': 887,\n",
       " 'blizzard': 888,\n",
       " 'pay': 889,\n",
       " 'large': 890,\n",
       " 'wow': 891,\n",
       " 'ladies': 892,\n",
       " 'appears': 893,\n",
       " 'centre': 894,\n",
       " 'village': 895,\n",
       " 'friends': 896,\n",
       " 'takes': 897,\n",
       " 'china': 898,\n",
       " '13': 899,\n",
       " 'emmerdale': 900,\n",
       " 'devastated': 901,\n",
       " 'declaration': 902,\n",
       " 'islam': 903,\n",
       " 'disea': 904,\n",
       " 'myanmar': 905,\n",
       " 'wanted': 906,\n",
       " 'thousands': 907,\n",
       " 'alone': 908,\n",
       " 'arsonist': 909,\n",
       " 'front': 910,\n",
       " 'climate': 911,\n",
       " 'else': 912,\n",
       " 'america': 913,\n",
       " 'property': 914,\n",
       " 'happened': 915,\n",
       " 'drive': 916,\n",
       " 'global': 917,\n",
       " 'experts': 918,\n",
       " 'trust': 919,\n",
       " 'ready': 920,\n",
       " 'vs': 921,\n",
       " 'film': 922,\n",
       " '05': 923,\n",
       " 'till': 924,\n",
       " 'friend': 925,\n",
       " 'reports': 926,\n",
       " 'green': 927,\n",
       " 'muslims': 928,\n",
       " 'mount': 929,\n",
       " 'fukushima': 930,\n",
       " 'driving': 931,\n",
       " 'favorite': 932,\n",
       " 'eye': 933,\n",
       " 'germs': 934,\n",
       " 'mad': 935,\n",
       " 'drake': 936,\n",
       " 'marks': 937,\n",
       " 'thursday': 938,\n",
       " 'downtown': 939,\n",
       " 'insurance': 940,\n",
       " 'mph': 941,\n",
       " 'british': 942,\n",
       " 'instead': 943,\n",
       " 'coaches': 944,\n",
       " 'issued': 945,\n",
       " 'flight': 946,\n",
       " 'quiz': 947,\n",
       " 'reddits': 948,\n",
       " 'virgin': 949,\n",
       " 'chile': 950,\n",
       " 'bring': 951,\n",
       " 'taking': 952,\n",
       " 'r': 953,\n",
       " 'behind': 954,\n",
       " 'four': 955,\n",
       " 'park': 956,\n",
       " 'sign': 957,\n",
       " 'following': 958,\n",
       " 'comes': 959,\n",
       " 'scared': 960,\n",
       " 'escape': 961,\n",
       " 'govt': 962,\n",
       " '100': 963,\n",
       " 'date': 964,\n",
       " 'hiring': 965,\n",
       " 'true': 966,\n",
       " 'theater': 967,\n",
       " 'gave': 968,\n",
       " 'gop': 969,\n",
       " 'added': 970,\n",
       " 'turn': 971,\n",
       " 'aint': 972,\n",
       " 'nagasaki': 973,\n",
       " 'outrage': 974,\n",
       " 'sounds': 975,\n",
       " 'ppl': 976,\n",
       " 'landing': 977,\n",
       " 'download': 978,\n",
       " 'york': 979,\n",
       " 'patience': 980,\n",
       " 'former': 981,\n",
       " 'madhya': 982,\n",
       " 'pradesh': 983,\n",
       " 'led': 984,\n",
       " 'gems': 985,\n",
       " 'funtenna': 986,\n",
       " 'ancient': 987,\n",
       " 'subreddits': 988,\n",
       " 'colorado': 989,\n",
       " 'awesome': 990,\n",
       " 'upon': 991,\n",
       " 'reported': 992,\n",
       " 'risk': 993,\n",
       " 'turned': 994,\n",
       " 'seeing': 995,\n",
       " 'moment': 996,\n",
       " 'thinking': 997,\n",
       " 'mode': 998,\n",
       " 'early': 999,\n",
       " 'pakistani': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df['text']\n",
    "y_train = train_df['target']\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_of_unique_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index  = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10          three people died from the heat wave so far\n",
      "11    haha south tampa is getting flooded hah wait a...\n",
      "12    raining flooding florida tampabay tampa 18 or ...\n",
      "13                flood in bago myanmar we arrived bago\n",
      "14    damage to school bus on 80 in multi car crash ...\n",
      "Name: text, dtype: object\n",
      "[[578, 56, 597, 19, 1, 275, 430, 28, 598], [824, 599, 2800, 8, 209, 2801, 3441, 650, 2, 630, 7, 196, 3, 599, 2800, 55, 155, 7, 291, 66, 55, 155, 7, 291, 66, 6741, 241], [2802, 241, 1695, 6742, 2800, 1444, 54, 1169, 600, 260, 728, 3442], [217, 3, 4452, 905, 46, 1696, 4452], [225, 4, 178, 382, 10, 3443, 3, 4453, 122, 86, 349]]\n"
     ]
    }
   ],
   "source": [
    "train_sequence = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "print(X_train[10:15])\n",
    "print(train_sequence[10:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Applying padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 20),\n",
       " array([578,  56, 597,  19,   1, 275, 430,  28, 598,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 20\n",
    "train_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "train_padded.shape ,train_padded[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoding token into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'three people died from the heat wave so far'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, '?') for idx in sequence])\n",
    "\n",
    "decode(train_sequence[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(num_of_unique_words, 32, input_length=max_length),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.GRU(128, dropout=0.5),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
    "    metrics=[\"accuracy\"]) \n",
    "\n",
    "history = model.fit(train_padded, y_train,\n",
    "                validation_split=0.2, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], c='b', label='train')\n",
    "plt.plot(history.history['val_loss'], c='r', label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], c='b', label='train')\n",
    "plt.plot(history.history['val_accuracy'], c='r', label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('tweets.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 3s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 1, 1, 1, 1],\n",
       " 0    1\n",
       " 1    1\n",
       " 2    1\n",
       " 3    1\n",
       " 4    1\n",
       " Name: target, dtype: int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(train_padded)\n",
    "pred = [1 if i > 0.5 else 0 for i in pred]\n",
    "pred[:5], y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7613, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7613, 1]"
     ]
    }
   ],
   "source": [
    "accuracy_score(y_train, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ['my house is on fire due to blast']\n",
    "test_sequence = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_padded)\n",
    "pred = [1 if i > 0.5 else 0 for i in pred]\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
